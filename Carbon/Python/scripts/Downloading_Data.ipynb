{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f51523",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "\n",
    "### Purpose\n",
    "This jupyter notebook highlights two different methods for accessing and downloading data from Ocean Observatories Initiative Carbon System instruments. The first method utilizes OOI's API to perform M2M (Machine-2-Machine) queries for data from the OOI THREDDS data server. The second method requests data from OOI's DataExplorer ERDDAP server.\n",
    "\n",
    "#### THREDDs Data\n",
    "The data served up via OpenDAP on OOI THREDDs servers are the same datasets which can be accessed via OOI's Data Portal at https://ooinet.oceanobservatories.org/. This is the source for accessing realtime or near-realtime data from OOI. \n",
    "\n",
    "\n",
    "#### Data Explorer\n",
    "Data Explorer is the new tool for exploring, discovering, and downloading data from OOI. It can be accessed via the web at https://dataexplorer.oceanobservatories.org/. Data Explorer hosts \"gold copy\" versions of OOI datasets, with all the relevant data stream merged into a single unified file. These datasets are hosted on the Data Explorer ERDDAP server at  However, Data Explorer currently only from the Data Explorer website, they currently can't be downloaded from the ERDDAP server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c3c9c",
   "metadata": {},
   "source": [
    "---\n",
    "## OOINet/THREDDs\n",
    "First, we are going to access and download data from OOI's Data Portal. Then we will do some dataset reprocessing to make the resulting data easier and more intuitive to work with. This portion of the notebook relies on some community tools which have been developed by OOI's Data Team members which simplify interacting with OOI's API. The two tools are the OOINet tool (https://github.com/reedan88/OOINet) and the Data Explorations Modules (https://github.com/oceanobservatories/ooi-data-explorations).\n",
    "\n",
    "This notebook provides an example on how to use the OOINet download tool to perform the following functions:\n",
    "* Search for datasets\n",
    "* Identify desired reference designator\n",
    "* Get the associated metadata for a given reference designator\n",
    "* Request netCDF datasets for a reference designator\n",
    "* Download the netCDF dataset to your local machine\n",
    "\n",
    "The key parameters which the OOI API requires is the \"reference designator.\" A reference designator may be thought of as a type of instrument located at a fixed location and depth. It is split up into the following three pieces:\n",
    "1. Subsite - this is the part of the array that the instrument is located at (e.g. Coastal Pioneer Inshore Surface Mooring CP03ISSM)\n",
    "2. Node - this is the part of the subsite that the instrument is attached to (e.g. the Surface Buoy on CP03ISSM as SDB12)\n",
    "3. Sensor - this is the number-letter combination that designates a particular class and series of instrument (e.g. the Pro-Oceanus CO2-Pro Atmosphere as 04-PCO2AA000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some standard python data processing and analysis packages\n",
    "import os, sys, datetime, pytz, re\n",
    "import dateutil.parser as parser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dask to make use of parallel computing to significantly speed up processing speed\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56146bae",
   "metadata": {},
   "source": [
    "#### Import the ```ooinet``` M2M toolbox\n",
    "This toolbox is publicly available at https://github.com/reedan88/OOINet. It should be cloned onto your machine and the setup instructions followed before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f8509",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/reedan88/ooinet/\")\n",
    "from ooinet import M2M\n",
    "from ooinet.utils import convert_time, ntp_seconds_to_datetime, unix_epoch_time\n",
    "from ooinet.Instrument.common import process_file, add_annotation_qc_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66830e5c",
   "metadata": {},
   "source": [
    "#### Import ```ooi_data_explorations``` toolbox\n",
    "This toolbox is publicly available at https://github.com/oceanobservatories/ooi-data-explorations. Similarly to the ```ooinet``` toolbox above, it should be installed onto your machine following the setup instructions before use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55181e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/areed/Documents/OOI/oceanobservatories/ooi-data-explorations/python/\")\n",
    "from ooi_data_explorations.common import get_annotations, get_vocabulary, load_gc_thredds\n",
    "from ooi_data_explorations.combine_data import combine_datasets\n",
    "from ooi_data_explorations.uncabled.process_pco2a import pco2a_datalogger \n",
    "from ooi_data_explorations.qartod.qc_processing import identify_blocks, create_annotations, process_gross_range, \\\n",
    "    process_climatology, woa_standard_bins, inputs, ANNO_HEADER, CLM_HEADER, GR_HEADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a83ee2",
   "metadata": {},
   "source": [
    "---\n",
    "## Search Datasets\n",
    "First, we can search the available OOI Reference Designators (i.e. \"refdes\" for short) on the following keys: **array**, **node**, **instrument**. Additionally, can request for \"**English_names**\", which will return the descriptive name for the associated array, node, and instrument. Below, we will search for the available CTD instruments on the Pioneer Array Central Surface Mooring.\n",
    "\n",
    "The major caveat with the search is, similar to searching on ERDDAP datasets, the search terms must be partial or full match based on OOI nomenclature. For example, we have to search for \"PCO2\", \"PCO2AA\", or the full instrument name \"04-PCO2AA\" if we are searching for the sea-surface pCO2 sensor. We can't search \"pco2\", \"carbon dioxide\" or other instrument terms.\n",
    "\n",
    "gold_copy = 'http://thredds.dataexplorer.oceanobservatories.org/thredds/catalog/ooigoldcopy/public/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd06c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments = M2M.search_datasets(array=\"CP03ISSM\", English_names=True)\n",
    "instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20b6e9",
   "metadata": {},
   "source": [
    "From the returned list of available instruments above, we can select a particular instrument using its **reference designator** (refdes for short):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ba746",
   "metadata": {},
   "outputs": [],
   "source": [
    "refdes = \"CP03ISSM-SBD12-04-PCO2AA000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f47be65",
   "metadata": {},
   "source": [
    "---\n",
    "## Metadata\n",
    "Next, we can query OOINet for the metadata associated with the selected reference designator. The metadata contains such valuable information such as the available methods and streams (which are required to download the data), the particleKeys (the data variable names), and the associated units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = M2M.get_metadata(refdes)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb59de",
   "metadata": {},
   "source": [
    "#### Sensor Parameters\n",
    "Each instrument returns multiple parameters containing a variety of low-level instrument output and metadata. However, we are interested in science-relevant parameters. We can identify the science parameters based on the preload database, which designates the science parameters with a \"data level\" of L1 or L2. \n",
    "\n",
    "Consequently, we will want to filter and group the metadata for a given reference designator to identify the relevant parameters. First, we query the preload database with the relevant metadata for a reference designator. Then, we filter the metadata for the science-relevant data streams based on the preload information. Then, we reduce the results by grouping by the stream parameter to get the stream-by-stream data, which will be useful when requesting data from OOINet for download. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bc96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_levels = M2M.get_parameter_data_levels(metadata)\n",
    "data_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88385ad",
   "metadata": {},
   "source": [
    "Filter the metadata based on the data levels for **L1** & **L2** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ed39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parameter_ids(pdId, pid_dict):\n",
    "    data_level = pid_dict.get(pdId)\n",
    "    if data_level is not None:\n",
    "        if data_level > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e094827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = metadata[\"pdId\"].apply(lambda x: filter_parameter_ids(x, data_levels))\n",
    "metadata = metadata[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f4091",
   "metadata": {},
   "source": [
    "Groupby based on the reference designator - method - stream to get the unique values for each data stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.groupby(by=[\"refdes\",\"method\",\"stream\"]).agg(lambda x: pd.unique(x.values.ravel()).tolist())\n",
    "metadata = metadata.reset_index()\n",
    "metadata = metadata.applymap(lambda x: x[0] if len(x) == 1 else x)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a1e83",
   "metadata": {},
   "source": [
    "This returns all of the methods and streams which have scientific data. For some datasets, such as the PCO2W or METBK datasets, we will need to do further cleaning to get rid of engineering and other metadata streams that do not contain relevant science data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f9f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = metadata[\"stream\"].apply(lambda x: False if \"blank\" in x else True)\n",
    "metadata = metadata[mask]\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a1da7e",
   "metadata": {},
   "source": [
    "---\n",
    "## Deployment Information\n",
    "When we searched for datasets, it returned a table which listed the available deployment numbers for each of the datasets. We can get much more detailed information on the deployments for a particular reference designator by requesting the deployment information from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = M2M.get_deployments(refdes=refdes)\n",
    "deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4e6c4",
   "metadata": {},
   "source": [
    "We'll go ahead and save the deployment data as a csv since it might be useful when working with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f103bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments.to_csv(f\"../data/{refdes}_deployments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441fab9d",
   "metadata": {},
   "source": [
    "---\n",
    "## Vocab Information\n",
    "Additionally, if we are interested in more detailed information on the location that the reference designator is assigned to, we can request the vocab information for the given reference designator. The vocab information includes some of the \"**English_names**\" info we requested when searching for datasets, as well as instrument model, manufacturer, and the descriptive names for the reference designator location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9810c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = M2M.get_vocab(refdes=refdes)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc162239",
   "metadata": {},
   "source": [
    "---\n",
    "## Calibration Information\n",
    "We can also request the calibration information for a given reference designator. Since individual instruments are swapped during each mooring deployment & recovery, the calibration coefficients for a reference designator are different for each deployment. The way OOI operates is that it loads all the available calibration coefficients for a given reference designator. Then, for each deployment, it finds the calibration coefficients with the most recent calibration date which most closely _precedes_ the start of the deployment. The result is a table, sorted by deployment number for a reference designator, with the uid of the specific instrument, its calibration coefficients, when the instrument was calibrated, and the source of the calibration coefficients.\n",
    "\n",
    "Now, the ```PCO2A``` does not happen to require calibration information by OOI to process and deliver data, so there are no calibration data available from OOINet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrations = M2M.get_calibrations_by_refdes(refdes, deployments)\n",
    "calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a9aa2",
   "metadata": {},
   "source": [
    "It is also possible to request the calibration history for a specific instrument by utilizing the **uid** of the instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_calibrations = M2M.get_calibrations_by_uid(uid)\n",
    "uid_calibrations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a2afa2",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Datasets\n",
    "The ultimate goal of the queries above were to identify what data streams(s) we are interested in, along with supporting metadata/calibration information, in order to request the to download. Now we want to be able to request those data streams and get the associated netCDF files. This process involves the following steps:\n",
    "1. Identify the methods and data streams for the selected reference designator\n",
    "2. Request the THREDDS server url for the data sets\n",
    "3. Get the catalog of datasets on the THREDDS server\n",
    "4. Parse the catalog for the desired netCDF files\n",
    "5. Download the identified netCDF files to a local directory\n",
    "\n",
    "Below, we script the above steps in order to download all of the available datasets. In the following section we will combine the data delivered via different methods (e.g. telemetered, recovered_host, recovered_inst) to generate a single combined dataset with the most complete data record available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_overlaps(ds, deployments):\n",
    "    \"\"\"Trim overlapping deployment data (necessary to use xr.open_mfdataset)\"\"\"\n",
    "    # --------------------------------\n",
    "    # Second, get the deployment times\n",
    "    deployments = deployments.sort_values(by=\"deploymentNumber\")\n",
    "    deployments = deployments.set_index(keys=\"deploymentNumber\")\n",
    "    # Shift the start times by (-1) \n",
    "    deployEnd = deployments[\"deployStart\"].shift(-1)\n",
    "    # Find where the deployEnd times are earlier than the deployStart times\n",
    "    mask = deployments[\"deployEnd\"] > deployEnd\n",
    "    # Wherever the deployEnd times occur after the shifted deployStart times, replace those deployEnd times\n",
    "    deployments[\"deployEnd\"][mask] = deployEnd[mask]\n",
    "    deployments[\"deployEnd\"] = deployments[\"deployEnd\"].apply(lambda x: pd.to_datetime(x))\n",
    "    \n",
    "    # ---------------------------------\n",
    "    # With the deployments info, can write a preprocess function to filter \n",
    "    # the data based on the deployment number\n",
    "    depNum = np.unique(ds[\"deployment\"])\n",
    "    deployInfo = deployments.loc[depNum]\n",
    "    deployStart = deployInfo[\"deployStart\"].values[0]\n",
    "    deployEnd = deployInfo[\"deployEnd\"].values[0]\n",
    "    \n",
    "    # Select the dataset data which falls within the specified time range\n",
    "    ds = ds.sel(time=slice(deployStart, deployEnd))\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datalogger(ds):\n",
    "    ds = process_file(ds)\n",
    "    ds = trim_overlaps(ds, deployments)\n",
    "    ds = pco2a_datalogger(ds)\n",
    "    gc.collect()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253defe",
   "metadata": {},
   "source": [
    "Filter out the \"metadata\" datastreams; use only the regular dataset and the water dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastreams = M2M.get_datastreams(refdes)\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e635c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = datastreams[\"stream\"].apply(lambda x: False if \"metadata\" in x or \"blank\" in x or \"power\" in x or \"air\" in x else True)\n",
    "datastreams = datastreams[mask]\n",
    "datastreams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec11dc",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Data\n",
    "To access data, there are two applicable methods. The first is to download the data and save the netCDF files locally. The second is to access and process the files remotely on the THREDDS server, without having to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469f731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the available datasets\n",
    "for index in datastreams[mask].index:\n",
    "    # Get the method and stream\n",
    "    method = datastreams.loc[index][\"method\"]\n",
    "    stream = datastreams.loc[index][\"stream\"]\n",
    "\n",
    "    # Get the URL - first try the goldCopy thredds server\n",
    "    thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=True)\n",
    "\n",
    "    # Get the catalog\n",
    "    catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "    # Clean the catalog\n",
    "    catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "    \n",
    "    # Get the links to the THREDDs server and load the data\n",
    "    dodsC = M2M.URLS[\"goldCopy_dodsC\"]\n",
    "    \n",
    "    # Not all datasets have made it into the goldCopy THREDDS - in that case, need to request\n",
    "    # from OOINet\n",
    "    if len(catalog) == 0:\n",
    "        # Get the URL - first try the goldCopy thredds server\n",
    "        thredds_url = M2M.get_thredds_url(refdes, method, stream, goldCopy=False)\n",
    "\n",
    "        # Get the catalog\n",
    "        catalog = M2M.get_thredds_catalog(thredds_url)\n",
    "\n",
    "        # Clean the catalog\n",
    "        catalog = M2M.clean_catalog(catalog, stream, deployments)\n",
    "\n",
    "        # Get the links to the THREDDs server and load the data\n",
    "        dodsC = M2M.URLS[\"dodsC\"]\n",
    "    \n",
    "    # Now load the data\n",
    "    if method == \"telemetered\":\n",
    "        tele_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        tele_files = [f for f in tele_files if \"blank\" not in f]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            tele_data = xr.open_mfdataset(tele_files, preprocess=preprocess_datalogger, parallel=True)\n",
    "    elif method == \"recovered_host\":\n",
    "        host_files = [re.sub(\"catalog.html\\?dataset=\", dodsC, file) for file in catalog]\n",
    "        host_files = [f for f in host_files if \"blank\" not in f]\n",
    "        print(f\"----- Load {method}-{stream} data -----\")\n",
    "        with ProgressBar():\n",
    "            host_data = xr.open_mfdataset(host_files, preprocess=preprocess_datalogger, parallel=True)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9386aa",
   "metadata": {},
   "source": [
    "**Combine the datasets into a single dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc584e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = combine_datasets(tele_data, host_data, None, None)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4a323",
   "metadata": {},
   "source": [
    "**Clean up workspace variables and free up memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9752c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_data.close()\n",
    "tele_data.close()\n",
    "del tele_data, host_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1463fc2",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aca05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_netcdf(f\"../data/{refdes}_combined.nc\", engine=\"h5netcdf\")\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8fc18",
   "metadata": {},
   "source": [
    "---\n",
    "## Annotations\n",
    "Annotations contain important qualitative assessments of data quality from the instrument operators. They may range from explanations for why data is missing for a given time period to information about biofouling or other data quality issues. Annotations can be downloaded from OOINet for a particular reference designator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the annotations for each reference designator\n",
    "annotations = M2M.get_annotations(refdes)\n",
    "annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f259695",
   "metadata": {},
   "source": [
    "Save the annotations to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae63acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.to_csv(f\"../data/{refdes}_annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fccdd9",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Explorer\n",
    "---\n",
    "The data from Data Explorer are hosted via ERDDAP. To interact with Data Explorer's ERDDAP, we'll utilize the python package ```erddapy```.\n",
    "\n",
    "When using the Data Explorer ERDDAP server, the other metadata we accessed via M2M above, such as the sensor vocab, deployment info, calibration information, etc. is NOT available. That metadata may currently only be accessed via the OOINet M2M API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bda6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from erddapy import ERDDAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf14192",
   "metadata": {},
   "source": [
    "**Data Explorer ERDDAP url**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataExplorer = \"http://erddap.dataexplorer.oceanobservatories.org/erddap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378ba30",
   "metadata": {},
   "source": [
    "Connect to the Data Explorer ERDDAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "erd = ERDDAP(server=dataExplorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25c199",
   "metadata": {},
   "source": [
    "Search for ```PHSEN``` on the Irminger Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c62631",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = erd.get_search_url(search_for=\"cp03issm pco2a\", \n",
    "                                protocol=\"tabledap\",\n",
    "                                response=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336b40d",
   "metadata": {},
   "source": [
    "Get the dataset ids for the available PHSEN datasets on the Irminger Sea Flanking Mooring A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids = pd.read_csv(search_url)[\"Dataset ID\"]\n",
    "dataset_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332df18",
   "metadata": {},
   "source": [
    "Download the dataset from ERDDAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428eaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset id of the instrument you want to download\n",
    "dataset_id = \"ooi-cp03issm-sbd12-04-pco2aa000\"\n",
    "\n",
    "# Get the download url\n",
    "download_url = erd.get_download_url(dataset_id=dataset_id, \n",
    "                                    protocol=\"tabledap\",\n",
    "                                    response=\"opendap\")\n",
    "\n",
    "# Set up the parameters for the dataset request from the ERDDAP server\n",
    "erd.dataset_id = dataset_id\n",
    "erd.response = \"nc\"\n",
    "erd.protocol = \"tabledap\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9112a91",
   "metadata": {},
   "source": [
    "Open the requested dataset using ```xarray```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fa83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = erd.to_xarray()\n",
    "ds = ds.swap_dims({\"obs\":\"time\"})\n",
    "ds = ds.sortby(\"time\")\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
